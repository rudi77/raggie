Nachgedacht fÃ¼r ein paar Sekunden


Below is a README-style overview you can drop into your projectâ€™s `README.md`. It walks through the RAG systemâ€™s architecture, core components, and high-level data flows.

---

## Retrieval-Augmented Generation (RAG) System

A modular Python framework for building a retrieval-augmented generation pipeline:

* **Ingest** documents of various formats (PDF, Markdown, text), optionally performing OCR and image description.
* **Chunk** large documents into manageable pieces via pluggable strategies (page-wise, header-based, token- or character-based).
* **Embed** each chunk into a high-dimensional vector using OpenAI or sentence-transformers.
* **Store** embeddings and metadata in a ChromaDB vector store.
* **Retrieve** the top-k most similar chunks at query time.
* **Generate** an answer with context using an LLM (e.g. OpenAI Chat models).

---

### ğŸ“¦ Project Structure

```
rag/  
â”œâ”€ core/  
â”‚  â”œâ”€ config.py            # Settings via Pydantic  
â”‚  â”œâ”€ interfaces.py        # Abstract contracts (IParser, IChunker, IEmbedderâ€¦)  
â”‚  â””â”€ models.py            # Document, Chunk, Vector, Prompt, Response, FinalAnswer  
â”‚  
â”œâ”€ ingestion/  
â”‚  â”œâ”€ document_loader.py   # Orchestrates loading and chunking via ChunkerFactory  
â”‚  â”œâ”€ advanced_pdf_loader.py  # Extracts text, tables, images + optional VisionModel  
â”‚  â”œâ”€ markdown_parser.py    # Converts raw markdown â†’ Document  
â”‚  â””â”€ ocr.py                # Azure Document Intelligence OCR processor  
â”‚  
â”œâ”€ chunking/  
â”‚  â”œâ”€ chunker.py           # BaseChunker, helper to assemble Chunk objects  
â”‚  â”œâ”€ token_chunker.py     # Fixed-size token/character splits  
â”‚  â”œâ”€ recursive_character_chunker.py  
â”‚  â”œâ”€ markdown_chunker.py  
â”‚  â”œâ”€ html_chunker.py  
â”‚  â”œâ”€ page_wise_chunker.py  
â”‚  â””â”€ chunker_factory.py   # Picks chunker by file extension or type  
â”‚  
â”œâ”€ embedding/  
â”‚  â”œâ”€ embedder.py          # BaseEmbedder, common vector wrapper logic  
â”‚  â”œâ”€ text_embedder.py     # sentence-transformers implementation  
â”‚  â””â”€ embeddings.py        # LangChain OpenAIEmbeddings wrapper  
â”‚  
â”œâ”€ store/  
â”‚  â”œâ”€ vector_store.py      # BaseVectorStore interface, similarity helper  
â”‚  â””â”€ chroma_store.py      # ChromaDB-backed persistence & search  
â”‚  
â”œâ”€ llm/  
â”‚  â”œâ”€ llm_client.py        # Wraps ChatOpenAI; builds prompts with retrieved chunks  
â”‚  â””â”€ vision_model.py      # Uses GPT-4 Vision (litellm) to describe images  
â”‚  
â””â”€ cli/  
   â””â”€ main.py              # Typer-based CLI: `ingest`, `query`, `clear`, `chunks`  
```

---

### ğŸ” High-Level Data Flow

1. **Ingestion**

   * CLI â†’ `DocumentLoader.load_documents(path)`
   * File-type loaders produce LangChainâ€†Documents (text + metadata).
   * Each Document is converted to a `core.models.Document` and passed to a chunker.

2. **Chunking**

   * `ChunkerFactory.get_chunker_for_file(...)` picks the right strategy.
   * Chunks carry `id`, `document_id`, content, and metadata (`start_index`, headers, page no., etc.).

3. **Embedding & Storage**

   * Each chunkâ€™s text is embedded via `EmbeddingModel` (OpenAI) or `TextEmbedder`.
   * Resulting vector + metadata stored in ChromaDB (`ChromaStore`).

4. **Querying**

   * CLI â†’ `LLMClient`
   * Query text â†’ embedding â†’ ChromaDB top-k vector search â†’ retrieve chunk texts.
   * Build a contextual prompt and call ChatOpenAI â†’ answer + cited sources.

---

### ğŸ”§ Configuration

All settings (API keys, database paths, OCR credentials, logging) live in `core/config.py` and can be overridden via environment variables or a `.env` file.

---

### ğŸš€ Quickstart

```bash
# Install dependencies
pip install -r requirements.txt  

# Ingest a folder of PDFs
rag.exe ingest ./data --chunk-size 1200 --chunk-overlap 200

# Ask a question
rag.exe query "What is retrieval-augmented generation?" -k 3 -m gpt-o4-mini

# List chunks for a document
rag.exe chunks myfile.pdf --top-k 5

# Wipe all stored vectors
rag.exe clear --force
```

This architecture keeps each concern isolated, making it easy to:

* Swap in new chunking strategies or embedding models
* Replace ChromaDB with another vector store
* Extend to additional file formats or vision/OCR back-ends


