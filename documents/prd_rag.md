Super, danke für die Klarstellungen. Ich erstelle jetzt ein ausführliches Product Requirements Document (PRD) für ein modulares, multimodales RAG-System in Python. Es wird den Fokus auf Entwicklerfreundlichkeit legen, PDF/Markdown/PNG-Dateien verarbeiten, verschiedene LLMs und Embeddings via LiteLLM unterstützen, lokal via Docker lauffähig sein und eine CLI bieten. DSGVO-Konformität wird ebenfalls berücksichtigt.

Ich mache mich an die Arbeit und melde mich gleich mit dem fertigen PRD.

# Produktanforderungsdokument: Retrieval-Augmented Generation (RAG) System

## Einleitung

Retrieval-Augmented Generation (RAG) bezeichnet ein KI-Architekturmuster, bei dem ein Large Language Model (LLM) durch eine externe Wissensbasis mit relevanten Daten ergänzt wird, bevor es eine Antwort generiert ([Want to Wipe Out GenAI Hallucinations? Get a RAG](https://ediscoverytoday.com/2024/02/22/want-to-wipe-out-genai-hallucinations-get-a-rag-artificial-intelligence-best-practices/#:~:text=It%E2%80%99s%20the%20process%20of%20optimizing,sources%20before%20generating%20a%20response)). Dadurch kann das Modell auf Informationen außerhalb seines ursprünglichen Trainingsstandes zugreifen und diese Fakten in die Antwort einfließen lassen. Ein wichtiges Merkmal dieses Ansatzes ist, dass die generierten Antworten Quellenangaben enthalten – ähnlich Fußnoten in einem Artikel – sodass Benutzer die Aussagen überprüfen können ([Want to Wipe Out GenAI Hallucinations? Get a RAG](https://ediscoverytoday.com/2024/02/22/want-to-wipe-out-genai-hallucinations-get-a-rag-artificial-intelligence-best-practices/#:~:text=Retrieval,a%20phenomenon%20sometimes%20called%20hallucination)). Dies erhöht die Transparenz und das Vertrauen in die Antworten und reduziert das Risiko von Halluzinationen (falschen oder erfundenen Aussagen des Modells) erheblich ([Want to Wipe Out GenAI Hallucinations? Get a RAG](https://ediscoverytoday.com/2024/02/22/want-to-wipe-out-genai-hallucinations-get-a-rag-artificial-intelligence-best-practices/#:~:text=Retrieval,a%20phenomenon%20sometimes%20called%20hallucination)). 

Das in diesem Dokument beschriebene RAG-System richtet sich an **Softwareentwickler** als Zielgruppe. Es soll einfach in bestehende Produkte integrierbar sein und Entwicklern ermöglichen, kontextsensitive und faktengestützte KI-Antworten bereitzustellen. Im Folgenden werden die Ziele, Anforderungen, Architektur und weiteren Aspekte dieses Systems detailliert beschrieben.

## Ziele

Die Hauptziele des RAG-Systems lassen sich wie folgt zusammenfassen:

- **Nahtlose Integration und Erweiterbarkeit:** Bereitstellung eines modularen Systems, das Entwickler mühelos in bestehende Anwendungen integrieren können. Neue Komponenten (z.B. alternative Retrieval- oder Chunking-Methoden) sollen leicht hinzugefügt werden können, um das System an verschiedene Anforderungen anzupassen.  
- **Multimodale Unterstützung:** Ermöglichung der Verarbeitung von **Text- und Bilddaten**, um Wissensinhalte aus unterschiedlichen Formaten zugänglich zu machen. Das System soll sowohl textuelle Informationen als auch Bilder als Wissensquelle nutzen können.  
- **Hohe Antwortqualität und Vertrauenswürdigkeit:** Generierung von Antworten, die durch Quellen belegt sind, um Anwendern maximalen Mehrwert und Vertrauen zu bieten. Halluzinationen sollen minimiert werden, indem das LLM stets mit relevantem externem Kontext versorgt wird. Zudem soll das System Mechanismen bieten, die Antwortqualität zu bewerten und gegebenenfalls Rückfragen zu stellen, falls die Eingabe unklar ist.  
- **Skalierbarkeit und Performance:** Verarbeitung auch großer Datenmengen durch effiziente Indexierung und Abfragemechanismen. Das System soll performant auf umfangreiche Wissensbasen skalieren können (ggf. mittels verteilten Verfahren), ohne die Antwortzeiten wesentlich zu beeinträchtigen.  
- **Sicherheit und Compliance:** Sicherstellung des Datenschutzes (DSGVO-Konformität) sowie moderner Sicherheitsstandards bei Speicherung, Zugriff und Betrieb. Sowohl die Daten der Wissensbasis als auch Benutzeranfragen/-antworten sollen vertraulich behandelt und geschützt übertragen bzw. gespeichert werden.

## Anforderungen

Im Folgenden werden die zentralen Anforderungen an das RAG-System beschrieben. Diese umfassen funktionale Anforderungen an Features und Verhalten sowie technische Vorgaben an Kompatibilität und Leistung.

- **Unterstützte Datenformate (Initial):** Das System muss mindestens die Formate **PDF**, **Markdown** und **PNG-Bilddateien** als Eingabedaten unterstützen. Dokumente in diesen Formaten sollen importiert und automatisch in ein durchsuchbares internes Format umgewandelt werden. PDFs und Markdown-Dateien werden hierzu in Text umgewandelt (inklusive Auslesen von Überschriften, Absätzen, Listen etc.), während **Bilder** aus PNG-Dateien entweder mittels OCR (für textenthaltende Bilder) oder durch inhaltliche Beschreibung/Captions aufbereitet werden. Ziel ist es, diese Formate von Anfang an abzudecken, da sie in vielen Wissensdatenbanken und Dokumentationen üblich sind. Die Architektur soll außerdem offen dafür sein, in Zukunft weitere Formate (z.B. DOCX, HTML) einzubinden.

- **Multimodale Datenverarbeitung:** Das System soll **Text- und Bilddaten** als Wissensquelle verarbeiten können. Für Textdaten werden LLM-Embeddings genutzt (siehe „LLM- und Embedding-Kompatibilität“), während Bilder entweder über **multimodale Embeddings** oder **Bild-zu-Text-Modelle** eingebunden werden. Konkret bedeutet dies: Für Bilder kann z.B. OpenAIs **CLIP-Modell** eingesetzt werden, um Bilder und Text in denselben Vektorraum abzubilden ([Images in RAGs](https://www.rearc.io/blog/images-in-rags#:~:text=,Captioning%20or%20describing)). Dadurch lassen sich Bilder anhand von Textbeschreibungen semantisch durchsuchen und umgekehrt. Alternativ oder ergänzend können Bilder durch **automatische Bildbeschreibungen** (Captions) in Text umgewandelt werden, der dann wie normaler Text indexiert wird ([Images in RAGs](https://www.rearc.io/blog/images-in-rags#:~:text=,multimodal%20LLMs%2C%20enable%20tailoring%20the)). Diese Caption-Methode hat den Vorteil, dass die generierten Texte mit jedem LLM verarbeitet werden können und sogar zusammenfassbar sind, während die semantischen **CLIP-Embeddings** direkt ein **auffindbares Abbild des Bildinhalts** liefern ([Images in RAGs](https://www.rearc.io/blog/images-in-rags#:~:text=,multimodal%20LLMs%2C%20enable%20tailoring%20the)) ([Images in RAGs](https://www.rearc.io/blog/images-in-rags#:~:text=,Image%20querying)). Beide Ansätze (CLIP für inhaltliches Auffinden von Bildern und Bild-zu-Text für Wissensextraktion aus Bildern) sollen unterstützt werden, um maximale Flexibilität bei der multimodalen Suche zu ermöglichen.

- **Unterstützte Retrieval-Strategien:** Das System muss mehrere **Retrieval-Methoden** für die semantische Suche unterstützen. Standardmäßig sollen sowohl **semantische Vektorsuche** (ähnlich einer reinen Embedding-Suche) als auch **hybride Suche** zur Verfügung stehen. Bei der semantischen Suche werden die Einträge rein nach inhaltlicher Ähnlichkeit der Embeddings zum Query-Embedding gerankt. Die **hybride Suche** hingegen kombiniert semantische Vektorsuche mit klassischer lexikalischer Suche (Schlüsselwort- oder BM25-Suche) ([A Comprehensive Hybrid Search Guide | Elastic](https://www.elastic.co/what-is/hybrid-search#:~:text=Hybrid%20search%20is%20the%20combination,by%20retrieving%20data%20using%20vector)). So werden sowohl die **Bedeutung** der Anfrage als auch exakte Schlüsselbegriffe berücksichtigt ([A Comprehensive Hybrid Search Guide | Elastic](https://www.elastic.co/what-is/hybrid-search#:~:text=Hybrid%20search%20is%20a%20powerful,techniques%20into%20a%20search%20algorithm)) ([A Comprehensive Hybrid Search Guide | Elastic](https://www.elastic.co/what-is/hybrid-search#:~:text=Hybrid%20search%20is%20the%20combination,retrieving%20data%20using%20vector%20representations)). Diese Kombination erhöht die Präzision der Ergebnisse, da Stärken beider Ansätze vereint werden. Die Architektur soll offen gestaltet sein, um weitere Retrievalstrategien einbinden zu können – z.B. rein boolesche Filter, regelbasierte Suchen oder zukünftige ML-basierte Methoden. Entwickler sollen mit minimalem Aufwand eigene Retrievalmodule (etwa für Spezialindexes oder Re-Ranking-Algorithmen) hinzufügen können.

- **Unterstützte Chunking-Strategien:** Eingabedokumente sollen vor der Indexierung in sinnvolle **Chunks** (Abschnitte) zerlegt werden. Das System muss hierzu verschiedene Chunking-Strategien anbieten, u.a. **token-basiert**, **semantik-basiert** und **layout-basiert**. Bei der **token-basierten** Strategie wird ein fester Fensterrahmen von z.B. *n* Tokens (mit optionalem Überlapp) gewählt, um lange Texte in Stücke zu schneiden ([Chunking Strategies for LLM Applications | Pinecone](https://www.pinecone.io/learn/chunking-strategies/#:~:text=Fixed)). Dies ist effizient und stellt sicher, dass jedes Chunk die maximale erlaubte Länge nicht überschreitet. Die **semantik-basierte** Chunking-Strategie hingegen orientiert sich an inhaltlichen Zusammenhängen – z.B. Satz- oder Abschnittsgrenzen – und versucht, semantisch kohärente Abschnitte zu bilden, selbst wenn diese variierende Länge haben. Fortgeschrittene Ansätze könnten hier Embedding-Ähnlichkeiten nutzen, um zusammengehörige Sätze oder Paragraphen zu clustern ([Evaluating Chunking Strategies for Retrieval | Chroma Research](https://research.trychroma.com/evaluating-chunking#:~:text=popular%20RecursiveCharacterTextSplitter%20often%20perform%20well,in%20practice%20when%20parametrized%20appropriately)). Die **layout-basierte** Strategie berücksichtigt strukturelle Elemente des Dokuments (z.B. Überschriften in Markdown oder visuelle Bereiche in PDF). So könnten etwa bei Markdown-Dateien die Chunks entlang der Überschriftenhierarchie oder Listenelemente geschnitten werden, um inhaltlich geschlossene Abschnitte zu erhalten ([Chunking Strategies for LLM Applications | Pinecone](https://www.pinecone.io/learn/chunking-strategies/#:~:text=language%20commonly%20used%20for%20formatting,For%20example)). Ähnlich könnte bei PDF-Dokumenten ein Chunk pro Absatz, Tabelle oder Seite erzeugt werden, je nachdem was am sinnvollsten ist. Wichtig ist, dass das System diese Strategien flexibel unterstützt und erweiterbar hält: Entwickler sollen neue Chunking-Strategien (z.B. durch Überschreiben einer Basisklasse oder via Plugin) implementieren können, um Optimierungen für spezifische Datenarten zu erzielen.

- **LLM- und Embedding-Kompatibilität:** Die Lösung soll mit **verschiedenen LLMs und Embedding-Modellen** zusammenarbeiten. Dazu wird eine Abstraktionsschicht via **LiteLLM** eingesetzt, die es ermöglicht, **sowohl OpenAI-Modelle als auch Open-Source-Modelle** über ein einheitliches Interface anzusprechen ([LiteLLM: Call every LLM API like it's OpenAI [100+ LLMs] | Y Combinator](https://www.ycombinator.com/companies/litellm#:~:text=LiteLLM%20is%20an%20open,in%20the%20OpenAI%20format)). LiteLLM bietet eine einheitliche API (kompatibel zum OpenAI-Format) für über 100 verschiedene LLM-Endpunkte, darunter OpenAI, Azure, AWS Bedrock, Cohere, Anthropic und lokale Modelle ([LiteLLM: Call every LLM API like it's OpenAI [100+ LLMs] | Y Combinator](https://www.ycombinator.com/companies/litellm#:~:text=LiteLLM%20is%20an%20open,in%20the%20OpenAI%20format)). Durch diese Entkopplung kann das RAG-System leicht zwischen unterschiedlichen LLM-Anbietern wechseln oder mehrere parallel nutzen (z.B. Fallback auf ein Open-Source-Modell, falls das OpenAI-API-Limit erreicht ist). Ebenso soll die Embedding-Berechnung austauschbar sein – initial z.B. OpenAI’s *text-embedding-ada-002* Modell für Texte und CLIP für Bilder, mit der Möglichkeit, künftig auch lokale Embedding-Modelle (wie SentenceTransformers oder HuggingFace-Modelle) einzubinden. Diese Kompatibilität garantiert, dass Nutzer nicht an einen einzigen Cloud-Anbieter gebunden sind und auch datenschutzfreundliche Alternativen (eigene gehostete Modelle) verwenden können.

- **Indexierung und Retrieval-Integration:** Das System soll einen vollständigen **Indexierungs- und Suchworkflow** integriert bereitstellen. Nachdem Dokumente gechunked und in Vektoren eingebettet wurden, werden diese in einer **Vektor-Datenbank** persistent gespeichert (siehe „Speicherung“). Bei einer Benutzeranfrage (Query) wird zur Laufzeit zunächst ein Query-Embedding erzeugt, um dann in der Vektor-Datenbank die relevantesten *k* Chunks abzurufen (Vektor-ähnlichkeitssuche). Wichtig ist, dass das System **Änderungen an den Quelldokumenten** erkennt und verarbeitet: Wenn ein Dokument aktualisiert wird oder neue Dokumente hinzugefügt/gelöscht werden, muss eine **Re-Indexierung** bzw. Anpassung des Index erfolgen. Dies kann durch einen Hintergrundprozess oder Trigger erfolgen, der beispielsweise bei Upload eines neuen Dokuments automatisch dessen Embeddings berechnet und dem Index hinzufügt (bzw. alte ersetzt). Die Entwickler sollen nicht manuell in die Indexpflege eingreifen müssen – das System garantiert Konsistenz zwischen Datenquelle und Index. Bei der Suche können neben der reinen Vektorähnlichkeit auch die oben genannten **hybriden Methoden** zum Tragen kommen (z.B. Filterung der Kandidaten per Schlüsselwort oder **Reranking** der gefundenen Chunks anhand von Relevanzfeedback/Heuristiken). All dies ist nahtlos im System integriert, sodass eine Entwickleranwendung lediglich Dokumente bereitstellen und Queries stellen muss.

- **Verarbeitung großer Datenmengen:** Das System soll Mechanismen bieten, um auch **sehr umfangreiche Dokumentbestände** performant zu verarbeiten. Dies betrifft sowohl die Indexierung (einmalige Verarbeitung aller Dokumente) als auch die Bearbeitung eingehender Anfragen. Mögliche Ansätze sind die Unterstützung von **Batch-Verarbeitung** oder verteilten Verfahren (ähnlich Map-Reduce), um Embeddings für sehr viele Dokumente parallel zu berechnen. Beispielsweise könnte die Indexierung großer Datenmengen auf mehrere Worker aufgeteilt werden, die jeweils einen Teil der Dokumente chunken und einbetten. Ebenso könnte bei extrem großen Vektordatenbanken ein **sharding** oder verteilte Datenbankknoten zum Einsatz kommen. In der ersten Phase ist angedacht, das System lokal auf einem einzelnen Knoten (Server) auszuführen, doch es sollte konzeptionell vorbereitet sein, auf verteilte Umgebungen zu skalieren. Auch die **Anfrageverarbeitung** sollte optimiert sein – etwa durch Vorbeschränkung des Suchraums mittels vorab berechneter invertierter Indizes oder einer hierarchischen Suche, um bei sehr großen Indexen die Latenz gering zu halten. Kurzum: das System muss skalierbar sein, damit es mit wachsenden Datenmengen und Nutzeranfragen effizient umgehen kann.

- **Antwortqualität und -aufbereitung:** Ein zentrales Merkmal des RAG-Systems ist die **hohe Qualität der generierten Antworten**. Dazu gehört, dass jede Antwort eine **Quellenangabe** enthält, die auf die ursprünglichen Dokumente verweist, aus denen die Informationen stammen. Die Ausgabe des LLM wird also um Zitate bzw. Referenzen ergänzt, so dass ein Endnutzer nachvollziehen kann, woher die Fakten stammen ([Want to Wipe Out GenAI Hallucinations? Get a RAG](https://ediscoverytoday.com/2024/02/22/want-to-wipe-out-genai-hallucinations-get-a-rag-artificial-intelligence-best-practices/#:~:text=Retrieval,a%20phenomenon%20sometimes%20called%20hallucination)). Dies kann beispielsweise in Form von nummerierten Fußnoten oder Inline-Referenzen geschehen, die auf Dokumenttitel, Abschnitte oder sogar konkrete Textstellen zeigen. Zusätzlich soll das System eine **Bewertung der Antwortqualität** ermöglichen. Entwickler oder Endnutzer könnten Feedback geben (etwa über einen Bewertungsmechanismus wie „Hilfreich“/„Nicht hilfreich“ oder Sternebewertungen), welches vom System erfasst wird. Dieses Feedback kann dann entweder dem Entwickler Aufschluss über nötige Anpassungen geben oder zukünftig automatisiert in das System einfließen (z.B. um Retrieval oder das Prompting des LLM anzupassen). Ein weiterer Aspekt der Antwortaufbereitung ist ein **Rückfrage-Mechanismus**: Falls das System feststellt, dass die Benutzerfrage mehrdeutig ist oder die gefundenen Informationen nicht ausreichen, soll es in der Lage sein, eine **Klärungsfrage** zurückzustellen. Beispielsweise könnte bei einer vagen Anfrage wie „Bericht erstellen“ eine Rückfrage kommen à la „Welchen Berichtstyp meinen Sie?“. Dies erhöht die Chance, im zweiten Anlauf die korrekte kontextspezifische Antwort zu liefern. Zusammengefasst fokussiert dieses Anforderungspaket darauf, dass die Endantworten korrekt, nachvollziehbar und optimiert für den Nutzer sind.

- **Speicherung (Vektordatenbank):** Für die Speicherung der Embeddings wird initial **ChromaDB** als Vektordatenbank eingesetzt. ChromaDB ist eine **Open-Source Vektordatenbank**, die speziell für KI-Anwendungen und Embeddings konzipiert wurde ([Chroma: Open Source Vector Database for LLM Applications – Intellyx – The Digital Transformation Experts – Analysts](https://intellyx.com/2023/10/27/chroma-open-source-vector-database-for-llm-applications/#:~:text=Chroma%20is%20an%20open%20source,building%20LLM%20based%20AI%20applications)). Sie kann Embeddings samt zugehöriger Metadaten effizient speichern und schnelle Ähnlichkeitssuchen (z.B. über Cosine Similarity) durchführen. ChromaDB bietet eine Python-API und lässt sich lokal betreiben, was den Anforderungen des prototypischen lokalen Deployments entspricht. Sollte ein Dokument geändert werden, unterstützt Chroma zudem **Upsert-Operationen**, um Embeddings zu aktualisieren, ohne den gesamten Index neu aufbauen zu müssen. Die Entscheidung für ChromaDB in der ersten Phase basiert auf dessen Einfachheit und Leistungsfähigkeit für Entwickler: es ist leicht einzurichten und bietet solide Performance für unsere vorgesehenen Datenmengen. Langfristig ist die Architektur aber auch hier nicht festgelegt – es sollte möglich sein, alternative Vektorstores wie FAISS, Weaviate oder Pinecone einzubinden, sofern diese besser zu speziellen Produktionsumgebungen passen. Wichtig für jetzt ist: Die Vektordatenbank läuft lokal eingebettet im Docker-Container mit und verwaltet persistente Embedding-Daten inklusive aller nötigen Indexe.

## Architektur

 ([Basic RAG Architecture with the Key Components | Spring of Realtime LLMs](https://kdag-iit-kharagpur.gitbook.io/realtime-llm/rag-and-llm-architecture/basic-rag-architecture-with-the-key-components)) *Abbildung 1: Konzeptuelle RAG-Architektur mit modularen Komponenten.* Die Abbildung veranschaulicht den grundsätzlichen Ablauf in der Systemarchitektur. Zunächst werden verschiedene **Quelldaten** (z.B. PDFs, Markdown-Dateien oder Bilder) in einen gemeinsamen Pool geladen (1). Ein **Ingestions- und Preprocessing-Modul** extrahiert den Inhalt dieser Daten, segmentiert sie in Chunks und wandelt sie in Vektorembeddings um. Diese Embeddings werden im **Vektorindex** (ChromaDB) gespeichert (2). Stellt ein Benutzer nun über die Schnittstelle (CLI oder später Web-UI) eine **Anfrage** (3), wird diese vom System entgegengenommen und ebenfalls in ein Embedding transformiert (4). Ein **Retrieval-Modul** durchsucht daraufhin den Vektorindex nach den passendsten Einträgen zur Anfrage – je nach Strategie semantisch, hybrid etc. – und fördert die relevantesten Kontext-Chunks zutage (5). Diese werden mit der ursprünglichen Anfrage zu einem **augmentierten Prompt** kombiniert und an das LLM gesendet. Das **LLM** (z.B. GPT-3.5 Turbo oder ein lokales Llama-Modell) generiert auf Basis des Prompts eine Antwort, die abschließend vom System mit Quellenangaben versehen und dem Benutzer präsentiert wird (6). Dieser modularisierte Ablauf trennt deutlich die Schritte Vorverarbeitung, Suche und Antwortgenerierung, was die Wartung und Erweiterbarkeit des Systems verbessert.

Die Architektur ist in **Python** implementiert und folgt einem modularen Aufbau, der eine lose Kopplung der Komponenten sicherstellt. Im Einzelnen bestehen die wichtigsten Module und Komponenten aus:

- **Daten-Ingestion und Vorverarbeitung:** Dieses Modul verantwortet den Import von Quelldokumenten. Für jede der unterstützten Eingabeformaten gibt es Parser: z.B. ein PDF-Parser (der Text via PDF-Reading-Bibliothek extrahiert), ein Markdown-Parser (der Markdown-Syntax in Klartext überführt) und ein Bild-Parser. Letzterer kann je nach Konfiguration entweder OCR für textlastige Bilder anwenden oder ein Bildbeschreibungssystem nutzen, um einen Text-**Caption** zu erzeugen. Im Anschluss an die Rohtextextraktion übernimmt die **Chunking-Komponente**: Sie teilt den Text (bzw. die Bildbeschreibung) anhand der gewählten Strategie in sinnvolle Segmente. Die Schnittstelle dieses Moduls ist so ausgelegt, dass neue Parser oder Chunking-Strategien leicht hinzugefügt werden können (etwa durch Registrieren einer neuen Klasse). Jedes Dokument wird dadurch in eine Menge von Chunks überführt, die zur Indexierung bereitstehen.

- **Embedding-Erstellung:** Die so gewonnenen Chunks werden an das Embedding-Modul übergeben. Dieses Modul nutzt die konfigurierten **Embedding-Modelle** – für Text z.B. den OpenAI-Embedding-Endpunkt oder einen lokalen Transformer, für Bilder z.B. CLIP – um aus jedem Chunk einen hochdimensionalen Vektor zu berechnen. Hier kommt die **LiteLLM-Schnittstelle** ins Spiel: Anstatt direkt einen spezifischen API-Client (etwa OpenAI) aufzurufen, schickt das Modul die Textdaten an LiteLLM, welches dann das aktuell gewählte LLM-API anspricht. So können Embeddings je nach Einstellung aus der Cloud oder lokal kommen, ohne dass das Embedding-Modul Änderungen benötigt. Die resultierenden Vektoren werden zusammen mit Metadaten (z.B. Dokument-ID, Abschnitt-Position, ggf. Titel) an das Speicher-Modul weitergereicht. Dieses Embedding-Modul ist ebenfalls austauschbar; falls etwa ein Unternehmen eigene domänenspezifische Embeddings nutzen möchte, kann man dieses Modul entsprechend erweitern.

- **Vektorindex und -Speicherung (ChromaDB):** Hier werden die vom Embedding-Modul gelieferten Vektoren persistent abgelegt. Wir nutzen ChromaDB als eingebettete Datenbank, die innerhalb der Python-Anwendung läuft. Chroma speichert jeden Vektor in einer **Collection**, die unserer gesamten Wissensbasis entspricht, und ermöglicht das Abfragen der ähnlichsten Vektoren zu einem gegebenen Query-Vektor. Zusätzlich speichert Chroma die Metadaten, damit bei einem Treffer später der ursprüngliche Dokumentinhalt und die Quelle referenziert werden können. Das **Indexformat** von Chroma (HNSW oder ähnliches für Approximate Nearest Neighbors) sorgt für schnelle Abfragen, selbst bei tausenden oder Millionen von Vektoren. Sollte ein Dokument gelöscht oder verändert werden, erhält das Speicher-Modul vom Ingestion-Modul ein Signal, um die entsprechenden Vektoreinträge zu entfernen bzw. zu aktualisieren (Re-Indexierung). So bleibt der Index konsistent. Die **Entscheidung für ChromaDB** begründet sich durch die Open-Source-Natur und die speziell auf Entwickler ausgerichtete Einfachheit des Systems ([Chroma: Open Source Vector Database for LLM Applications – Intellyx – The Digital Transformation Experts – Analysts](https://intellyx.com/2023/10/27/chroma-open-source-vector-database-for-llm-applications/#:~:text=Chroma%20is%20an%20open%20source,building%20LLM%20based%20AI%20applications)). Außerdem unterstützt Chroma schon out-of-the-box Python und kann auch multimodale Embeddings (Text/Bild) verwalten, was unserer Anforderung genau entspricht ([Chroma: Open Source Vector Database for LLM Applications – Intellyx – The Digital Transformation Experts – Analysts](https://intellyx.com/2023/10/27/chroma-open-source-vector-database-for-llm-applications/#:~:text=Chroma%20is%20an%20open%20source,building%20LLM%20based%20AI%20applications)) ([Chroma: Open Source Vector Database for LLM Applications – Intellyx – The Digital Transformation Experts – Analysts](https://intellyx.com/2023/10/27/chroma-open-source-vector-database-for-llm-applications/#:~:text=The%20Chroma%20database%20stores%20embeddings,provides%20tools%20to%20search%20embeddings)).

- **Abfrageverarbeitung und Retrieval-Modul:** Sobald ein Benutzer (oder eine angebundene Anwendung) eine **Frage** an das System stellt, übernimmt dieses Modul. Zunächst wird die natürliche Spracheingabe (Query) analog zur Dokumentenverarbeitung in ein Embedding umgewandelt (wieder via LiteLLM und den gleichen Embedding-Modellen, um im selben Vektorraum zu bleiben). Anschließend führt das Retrieval-Modul eine Suche im Vektorindex durch. Standardmäßig wird hierbei eine **k-NN Suche** auf den Embeddings gemacht, um die Top-k ähnlichsten Chunks zu finden. Je nach Konfiguration kann das Retrieval-Modul aber auch weitere Schritte ausführen: Beispielsweise könnten die gefundenen Chunks noch einem **Filter** unterzogen werden, der sicherstellt, dass sie unterschiedliche Dokumente abdecken (um Diversität zu erhöhen), oder es könnte ein **lexikalischer Abgleich** mit der Query stattfinden, um exakt passende Dokumente höher zu gewichten (hybride Suche). Dieses Modul kapselt die gesamten Suchstrategien. Es ist so gestaltet, dass man neue Strategien implementieren kann – etwa eine Kombination aus Vektor- und Datenbankabfragen oder den Einsatz eines **Re-Ranker-Modells**, das die Top-10 Ergebnisse nochmals inhaltlich bewertet. Die Ausgabe des Retrieval-Moduls ist jedenfalls der **Kontext**: eine Sammlung von z.B. 3–5 relevanten Textausschnitten (Chunks) samt Quell-Infos, die für die Beantwortung der Frage nützlich sind.

- **LLM-Generierungsmodul:** Mit dem vom Retrieval gelieferten Kontext konstruiert das System nun den Prompt für das LLM. In diesem Prompt wird typischerweise die Benutzerfrage zusammen mit den eingefügten relevanten Kontextstücken formuliert (ggf. mit einer Instruktion an das LLM, unbekannte Informationen nicht zu halluzinieren). Dieses Modul ruft dann – wiederum über LiteLLM – das konfigurierte **LLM** zur Generierung der Antwort auf. Durch LiteLLM kann dies ein Aufruf der OpenAI-API sein (z.B. GPT-4) oder ein lokaler Modellaufruf über eine OpenAI-kompatible API (etwa zu *llama.cpp* oder *Ollama*). Das Generierungsmodul erhält vom LLM einen oder mehrere Antwortvorschläge. Es wählt den passenden aus (bei Bedarf nachscoret anhand von Präferenzen, z.B. Kürze der Antwort) und übergibt die rohe Antwort an die Postprozessierung. Wichtig: Die Generierung erfolgt **streaming-fähig**, d.h. bei längeren Antworten könnte das System die ersten Tokens sofort ausgeben, während das LLM noch die restliche Antwort produziert – dies ist für eine spätere Echtzeit-Weboberfläche relevant, wird aber initial in der CLI einfacher gehalten.

- **Antwort-Postprozessierung:** Bevor die Antwort an den Nutzer zurückgeht, wird sie noch mit **Quellenangaben** versehen. Das Modul nimmt die vom LLM erzeugte Antwort und die beim Retrieval verwendeten Dokument-IDs und formatiert daraus Referenzen. Da unser System weiß, aus welchen Dokumenten die Kontext-Chunks stammten, kann es z.B. eckige Klammern mit Ziffern an relevanten Aussagen der LLM-Antwort anfügen und diese Ziffern dann den Dokumentnamen bzw. URLs zuordnen (ähnlich wie in diesem Dokument die Zitation erfolgt). Dieser Schritt kann auch teilweise schon im Prompt mit dem LLM vorbereitet worden sein (man kann ein LLM bitten, selbst Zitate zu setzen), aber um konsistente und korrekte Quellenangaben sicherzustellen, behandelt das System dies nachgelagert oder in einem Validierungsschritt. Zusätzlich kann hier das **Feedback-Logging** stattfinden: die Antwort (und optional das Benutzerfeedback, wenn vorhanden) wird geloggt, um später die Evaluation zu ermöglichen. Schließlich wird die fertig formattierte Antwort über die Schnittstelle zurückgegeben.

- **Erweiterbarkeit und Modularität:** Die gesamte Architektur ist darauf ausgelegt, **neue Module** oder Implementationen leicht einbinden zu können. Alle wichtigen Komponenten – Dateiparser, Chunker, Embedder, Retriever, LLM-Client – sind als **austauschbare Services** implementiert, die über wohl definierte Schnittstellen miteinander kommunizieren. Beispielsweise könnte ein Entwickler einen alternativen **Retriever** schreiben, der einen Wissensgraphen abfragt, und ihn einfach anstelle des Vektor-Retrievers konfigurieren. Oder ein anderer **Speicher** könnte verwendet werden, indem man das Speicher-Interface gegen einen Wrapper für ElasticSearch oder FAISS austauscht. Diese Flexibilität wird durch klar getrennte Verantwortlichkeiten und die Verwendung von Entwurfsmustern (z.B. Strategy Pattern für Retrieval- und Chunking-Strategien) erreicht. So bleibt der Kern des Systems erweiterbar, ohne dass Änderungen in einer Komponente breite Auswirkungen auf andere Teile haben.

Zusammengefasst besteht die Architektur aus einem **End-to-End-Pipeline** von der Datenerfassung bis zur Antwortgenerierung, modular aufgebaut in Python. Die Entscheidung für Python begünstigt die Nutzung existierender Ökosysteme (z.B. `langchain`, `haystack` oder ähnliche Bibliotheken) und die einfache Integration von KI-Modellen und Datenbanken. Trotz hoher Modularität läuft das System innerhalb eines gemeinsamen Prozesses/Containers, was Latenz durch zwischenprozess-Kommunikation minimiert. Künftig könnte diese Architektur auch über Microservices verteilt werden (z.B. ein separater Vektordatenbank-Service, separater LLM-Service), aber initial ist eine monolithische, jedoch modulare Python-Anwendung vorgesehen, die der Einfachheit halber in einem Docker-Container betrieben wird.

## Sicherheitsaspekte

Sicherheit und Datenschutz genießen hohe Priorität bei der Entwicklung des RAG-Systems, insbesondere da es in Unternehmenskontexte integriert werden soll. Folgende Sicherheitsaspekte werden berücksichtigt:

- **DSGVO-Konformität (GDPR Compliance):** Das System wird so konzipiert, dass es den Anforderungen der EU-Datenschutz-Grundverordnung entspricht, sofern personenbezogene Daten verarbeitet werden. Konkret bedeutet dies u.a. Datensparsamkeit (es werden nur für die Funktion absolut notwendige personenbezogene Daten im System gehalten, z.B. vielleicht Benutzer-IDs für Feedback, aber keine Klartext-Namen), Zweckbindung und Löschkonzepte. Benutzeranfragen und die daraufhin genutzten Daten bleiben im System nur temporär gespeichert, sofern nicht anders erforderlich. Bei Integration in bestehende Produkte wird darauf geachtet, dass Daten lokal oder in vom Kunden kontrollierten Umgebungen verbleiben (zunächst ist ja nur ein lokaler Einsatz vorgesehen). Für einen späteren Cloud-Betrieb werden Mechanismen vorgesehen, die z.B. eine **Datenverarbeitungsvereinbarung** (Data Processing Agreement) mit dem Kunden ermöglichen und die Auswahl des **Datenstandorts** (EU-Server) erlauben ([Security & Compliance](https://www.concord.app/security-compliance/#:~:text=Image)). Kurz gesagt: Das System soll „privacy by design“ umsetzen – von der Architektur (z.B. optional anonymisierte Logging-Daten) bis zur Betriebsphase (Möglichkeit zur vollständigen Datenlöschung auf Anfrage etc.).

- **Transportverschlüsselung (TLS):** Alle Datenübertragungen, insbesondere wenn in späteren Ausbaustufen Dienste entkoppelt werden (z.B. eine Web-UI oder separate Backend-Services), erfolgen ausschließlich über verschlüsselte Verbindungen. In der initialen lokalen Docker-Umgebung kommunizieren die Module intern, doch sobald z.B. eine Web-API angeboten wird, wird **HTTPS (TLS 1.2+)** verwendet. Dies stellt sicher, dass keine sensiblen Inhalte (Benutzerfragen oder Dokumenteninhalte) im Klartext über Netzwerke übertragen werden. Standard ist hier die Verwendung etablierter Bibliotheken/Frameworks, die TLS unterstützen, so dass gängige Sicherheitsprotokolle eingehalten werden (inklusive Zertifikatsprüfung). Im Falle einer Client-Server-Architektur gilt so: **Daten in Transit** sind stets geschützt ([Security & Compliance](https://www.concord.app/security-compliance/#:~:text=All%20data%20in%20Concord%20is,encryption%20for%20data%20at%20rest)).

- **Speicherverschlüsselung:** Für Daten, die auf persistenten Speichermedien liegen, wird ebenfalls Verschlüsselung vorgesehen. Insbesondere der Vektorindex könnte sensible Informationen enthalten (Teile von vertraulichen Dokumenten in Vektorform). Daher soll bei produktivem Einsatz die Option bestehen, die Speichermedien mit **AES-256** Verschlüsselung abzusichern (z.B. durch Verschlüsselung des Dateisystems oder wenn ein externer Datenbankdienst genutzt wird, durch dessen Verschlüsselungsfeature) ([Security & Compliance](https://www.concord.app/security-compliance/#:~:text=All%20data%20in%20Concord%20is,encryption%20for%20data%20at%20rest)). In vielen Umgebungen (z.B. Cloud-Storage oder Datenbanken) ist AES-256-Verschlüsselung mittlerweile Standard, und unser Systemdesign stellt sicher, dass diese genutzt wird. Somit sind **Daten at Rest** (im Ruhezustand) ebenso geschützt wie die übertragenen Daten.

- **Zugriffssteuerung und Rollen:** Das System wird eine **rollenbasierte Zugriffskontrolle (RBAC)** implementieren, sobald mehrere Benutzer oder Dienste darauf zugreifen. In der ersten Phase (lokale CLI) ist das weniger relevant, doch perspektivisch – z.B. bei Bereitstellung einer Web-Oberfläche oder eines APIs – müssen unterschiedliche Berechtigungen möglich sein. Zum Beispiel könnte es Administrator-Rollen geben, die neue Dokumente hinzufügen oder das System rekonfigurieren dürfen, während Endnutzer nur Abfragen stellen können. Die Authentifizierung und Autorisierung wird nach dem Prinzip der **minimalen Rechte** umgesetzt, d.h. jede Komponente und jeder Benutzer erhält nur die Zugriffsrechte, die er unbedingt benötigt ([Security & Compliance](https://www.concord.app/security-compliance/#:~:text=Concord%20implemented%20a%20SOC%202,and%20the%20least%20privilege%20principle)). Bereits vorhandene Authentifizierungssysteme (etwa eines vorhandenen Produktes, in das das RAG-System integriert wird) sollen angebunden werden können. Darüber hinaus werden Security-Best-Practices verfolgt wie starke Passwörter/Keys, optional 2-Faktor-Authentifizierung für Admin-Zugriffe und regelmäßige Überprüfung der Zugangskontrollen.

- **Sicherheit des Betriebs (Container & Code):** Da das System zunächst via Docker-Container ausgeliefert wird, achten wir darauf, dass der Container keine unnötigen offenen Ports oder Dienste hat. Alle nicht benötigten Netzwerkservices werden abgeschaltet. Externe Abhängigkeiten (Packages) werden auf Sicherheitslücken geprüft und regelmäßig aktualisiert. Der Container läuft mit minimalen Rechten auf dem Host-System. Zukünftige Cloud-Deployments würden auf zertifizierten Cloud-Infrastrukturen laufen (mit Compliance wie ISO 27001, SOC2 etc., falls erforderlich) ([Security & Compliance](https://www.concord.app/security-compliance/#:~:text=Concord%E2%80%99s%20servers%20are%20hosted%20on,HIPAA%2C%20FIPS%2C%20FISMA%2C%20and%20CSA)). Außerdem wird Logging sensibel gehandhabt: Sensitive Inhalte werden nicht in Logs geschrieben oder zumindest maskiert. Penetrationstests und Code-Reviews werden vor Produktivnahme stattfinden, um gängige Schwachstellen (Injection, XSS, etc. – soweit in unserem Kontext relevant) auszuschließen. 

Zusammenfassend stellt das System sicher, dass sowohl technische Sicherheit (Verschlüsselung, Zugriffsschutz) als auch datenschutzrechtliche Vorgaben berücksichtigt werden, damit es vertrauensvoll in unternehmenskritischen Umgebungen eingesetzt werden kann.

## Schnittstellen

In der Anfangsversion wird der Zugriff auf das System vor allem über **Kommandozeilenwerkzeuge (CLI)** ermöglicht, später sind jedoch weitere Schnittstellen – insbesondere eine Web-Oberfläche – geplant. Die Schnittstellen gliedern sich in:

- **Command-Line Interface (CLI):** Eine zunächst primäre Nutzerschnittstelle ist ein CLI-Tool, über das Entwickler das System bedienen können. Über Kommandozeilenbefehle lässt sich zum Beispiel ein neuer Dokumentenbestand indexieren (z.B. `rag index <Ordner>`) oder eine Abfrage stellen (`rag query "Frage"`). Die Antworten mit Quellenangaben werden dann in der Konsole ausgegeben. Die CLI eignet sich für schnelle Tests, Automatisierungen (z.B. Skripte) und Integrationen in Entwickler-Workflows. Sie soll mit sinnvollen Unterbefehlen und Optionen ausgestattet sein (ähnlich gängigen Tools wie `git` oder `langchain` CLI), sodass man z.B. den Status des Vektorspeichers prüfen oder Evaluationstests ausführen kann. Für die CLI wird Python’s `argparse` oder eine Bibliothek wie Typer genutzt, um eine benutzerfreundliche Hilfe und Autocompletion zu bieten. Diese Schnittstelle erlaubt Entwicklern in frühen Phasen, das System manuell zu steuern und auszuprobieren.

- **Programmierbare Schnittstelle (API/SDK):** Da das Ziel die Integration in andere Produkte ist, wird es eine Programmierschnittstelle geben. In der ersten Implementation kann dies einfach die Python-API des Systems selbst sein – Entwickler importieren ein Modul (z.B. `rag_system`) und können dann Funktionen aufrufen wie `index_documents(path)` oder `answer_query(question)`, welche die entsprechenden Operationen durchführen und Ergebnisse als Objekte zurückliefern. Langfristig ist auch eine **RESTful API** oder GraphQL-API denkbar, sodass unabhängig von der Programmiersprache via HTTP auf das System zugegriffen werden kann. Dieser Schritt könnte mit dem Web-Interface zusammenfallen (das Web-Backend stellt dann die API bereit). Wichtig ist, dass die Integration möglichst simpel abläuft: Die Entwickler sollen nicht gezwungen sein, sich mit den Details der Pipeline auseinanderzusetzen. Eine einzige Methode, der man eine Frage übergibt und die ein Antwortobjekt (mit Text und Quellenliste) zurückgibt, kapselt intern den gesamten Prozess. Ebenso sollte es eine einfache Möglichkeit geben, Dokumente hinzuzufügen oder zu aktualisieren, z.B. durch Bereitstellen eines Dateiordners oder eines Upload-Endpunkts. Die API muss gut dokumentiert sein und Versionierung unterstützen, damit Integratoren sich darauf verlassen können.

- **Web-Oberfläche (geplant):** In einer späteren Ausbaustufe ist ein **Web-Frontend** vorgesehen, das eine benutzerfreundliche Oberfläche für das Stellen von Fragen und ggf. das Verwalten der Wissensdatenbank bietet. Diese Web-UI könnte als einfaches Chat-Interface gestaltet sein, in dem der Benutzer Fragen eingibt und vom System Antworten mit Zitatverweisen erhält (vergleichbar mit ChatGPT, ergänzt um die Quellen). Darüber hinaus könnten Admin-Nutzer über die Web-Oberfläche Dokumente hochladen, löschen oder den Systemstatus einsehen. Denkbar ist auch, das Nutzer-Feedback hier zu integrieren (z.B. „War diese Antwort hilfreich? [Ja/Nein]“ Buttons unter jeder Antwort). Für die Umsetzung kämen Web-Frameworks wie Flask/FastAPI (für das Backend) und ein Frontend-Framework (React, Vue.js) infrage. Da die erste Version aber lokal per Docker läuft, wird diese Komponente zunächst zurückgestellt – die Architektur hält den Übergang zur Web-UI jedoch offen. Die Web-Oberfläche wird insbesondere dann wichtig, wenn das System nicht nur von Entwicklern, sondern direkt von Fachexperten oder Endanwendern benutzt werden soll, die kein CLI bedienen möchten.

- **Monitoring & Admin Interface:** Zwar nicht explizit gefordert, aber der Vollständigkeit halber sei erwähnt, dass für den produktiven Einsatz auch gewisse Monitoring-Schnittstellen sinnvoll sind. Zum Beispiel könnte ein *Dashboard* existieren (oder zumindest Log-Ausgaben abrufbar sein), das anzeigt, wie viele Dokumente indexiert wurden, wie groß der Vektorindex ist, durchschnittliche Antwortzeiten etc. Solche Infos könnten im Web-Interface für Admins sichtbar sein oder via CLI abgefragt werden. Außerdem sollte es administrative Kommandos geben, z.B. zum Neuaufbau des Index, zum Flushen des Vektorspeichers, zum Export/Import der Daten oder zum Update der KI-Modelle. Diese Aspekte fließen in die Schnittstellengestaltung mit ein, damit das System auch wartbar und beobachtbar ist.

Zusammengefasst bietet das System zunächst eine **CLI für Entwickler** und eine **interne API**. Später folgt eine **Web-Oberfläche** für Endnutzer, wobei darunterliegend eine erweiterte Web-API liegen wird. Durch diese gestuften Zugriffsmöglichkeiten kann das System sowohl in automatisierte Abläufe eingebunden werden als auch interaktiv von Menschen genutzt werden.

## Evaluierung

Um die Wirksamkeit und Qualität des RAG-Systems sicherzustellen, werden sowohl **quantitative Metriken** als auch **qualitative Methoden** zur Evaluierung herangezogen. 

Auf der quantitativen Seite werden klassische Informationsretrieval-Kennzahlen angewandt, um den Erfolg des Retrieval-Schritts zu messen. Dazu zählen insbesondere **Precision** (Genauigkeit) und **Recall** (Vollständigkeit) der zurückgelieferten Dokumenten-Chunks im Verhältnis zu einer Benchmark. Diese Metriken können auf Chunk-Ebene bzw. tokenbasiert berechnet werden – etwa: Wie viele der wirklich relevanten Tokens hat das System in den Top-k Resultaten gefunden (Recall), und wie viel Prozent der gelieferten Tokens waren tatsächlich relevant (Precision) ([Evaluating Chunking Strategies for Retrieval | Chroma Research](https://research.trychroma.com/evaluating-chunking#:~:text=Motivated%20by%20these%20limitations%2C%20we,the%20basis%20of%20retrieved%20tokens)). Eine weitere Kennzahl, die erwähnt wird, ist die **Intersection-over-Union (IoU)** auf Token-Ebene ([Evaluating Chunking Strategies for Retrieval | Chroma Research](https://research.trychroma.com/evaluating-chunking#:~:text=Motivated%20by%20these%20limitations%2C%20we,the%20basis%20of%20retrieved%20tokens)). IoU, verwandt mit dem Jaccard-Index, misst den Anteil der Überlappung zwischen der Menge relevanter Tokens und der Menge der in den Ergebnissen enthaltenen Tokens. Diese fein granularen Metriken erlauben es, verschiedene Chunking- und Retrieval-Strategien objektiv zu vergleichen und zu optimieren. Beispielsweise könnte man in einem Testdatensatz (mit bekannten relevanten Textstellen pro Frage) überprüfen, ob semantisches Chunking einen höheren Recall erzielt als rein tokenbasiertes. Ebenso lassen sich Antwortgenerierungskomponenten bewerten, z.B. mittels BLEU- oder ROUGE-Scores gegen erwartete Antworten, falls ein goldener Datensatz vorliegt.

Neben diesen quantitativen Maßen ist auch **qualitatives Nutzerfeedback** essentiell. Das System soll daher Benutzer ermöglichen, Feedback zur Qualität der Antworten zu geben (z.B. über Daumen hoch/runter oder Freitextkommentare in einer Testphase). Dieses Feedback wird gesammelt und analysiert, um Muster zu erkennen: Treten bestimmte Fehlantworten häufiger auf? Werden bestimmte Dokumente fälschlich zitiert? Solches Feedback kann in zukünftige Verbesserungen einfließen, etwa indem man die Prompt-Vorlage des LLM anpasst oder zusätzliche Filter vor der Antwortgenerierung einführt. In frühen Entwicklungsphasen werden wir voraussichtlich manuelles Testing durchführen: Domänenexperten stellen Fragen und bewerten, ob die Antworten korrekt und hilfreich sind. 

Ein möglicher Evaluierungsworkflow wäre: eine Reihe von Testfragen wird an das System gestellt; für jede wird geprüft, ob die Antwort faktisch richtig ist (ggf. durch einen Menschen) und ob die angegebenen Quellen tatsächlich die Grundlage der Antwort enthalten. So könnten wir eine **Trefferquote** bestimmen. Zudem könnten A/B-Vergleiche zwischen verschiedenen Konfigurationen laufen – z.B. Vektor-Suche vs. Hybrid-Suche – um zu sehen, welche bessere Ergebnisse liefert.

Sobald das System produktiv im Einsatz ist, können auch laufend Metriken wie die durchschnittliche Bearbeitungsdauer pro Anfrage, die Ableitungsrate von Rückfragen (wie oft musste das System Rückfragen stellen) und die Verteilung der Feedbackbewertungen erfasst werden. Diese **Betriebsmetriken** geben Hinweise auf Performance-Probleme oder Verbesserungspotenziale im Nutzererlebnis.

Schließlich ist es denkbar, automatisierte Evaluationsmethoden einzusetzen, z.B. die Antworten des Systems von einem zweiten LLM beurteilen zu lassen oder einen sogenannten *truthfulness*-Score zu berechnen. Primär bleibt jedoch der **Precision/Recall**-Ansatz für das Retrieval und echtes Nutzerfeedback für die Antwortqualität ausschlaggebend.

Die Evaluierungsergebnisse werden genutzt, um iterativ die Systemparameter und -module zu verfeinern. Beispielsweise könnte ein schlechter Recall auf zu kleine Chunk-Größen hindeuten – dann würde man die Chunking-Strategie anpassen. Oder negatives Nutzerfeedback könnte zeigen, dass Quellenangaben unklar sind – dann verbessert man die Darstellung der Zitate. Durch diesen kontinuierlichen Verbesserungsprozess wird sichergestellt, dass das RAG-System im praktischen Einsatz robust, zuverlässig und nützlich bleibt.

## Ausblick

In der aktuellen Planung wird das RAG-System in einer ersten Version lokal (on-premise in Docker) bereitgestellt und erfüllt die genannten Kernanforderungen. Es gibt jedoch bereits eine Roadmap für Erweiterungen und Verbesserungen in der Zukunft:

- **Weitere Datenformate und Datenquellen:** Perspektivisch soll die Liste der unterstützten Formate ausgebaut werden. Formate wie **DOCX (Word-Dokumente)**, **CSV/Excel** oder **HTML-Webseiten** stehen hoch im Kurs, da in realen Wissensbasen häufig solche Dateien vorkommen. Zudem könnte die Unterstützung von **strukturierter Daten** (Datenbanken, Wissensgraphen) eine sinnvolle Erweiterung sein, um auch tabellarisches oder kontextuales Wissen abzudecken. Der modulare Parser-Aufbau erleichtert das Hinzufügen neuer Formate erheblich. Auch **Echtzeit-Datenquellen** (z.B. ein Live-Crawl einer Website oder Streams) sind denkbar, sodass das System nicht nur statische Offline-Dokumente, sondern auch dynamische Informationsquellen verarbeiten kann.

- **Erweiterte multimodale Fähigkeiten:** Über Bilder hinaus ließe sich in Zukunft auch **Audio** oder **Video** einbinden. Zum Beispiel könnten Audioaufnahmen via Spracherkennung in Text umgewandelt und indexiert werden. Bei Videos könnte man Bild und Ton kombinieren: Frames durch ein Bild-Embedding erschließen und gesprochenen Inhalt transkribieren. Während dies aktuell nicht priorisiert ist, ist die Architektur offen genug, um weitere Modalitäten aufnehmen zu können. Gerade mit dem Aufkommen leistungsfähiger multimodaler Modelle (die Text, Bild und Audio gemeinsam verstehen) könnte das System später in diese Richtung erweitert werden.

- **Cloud-Service und skalierte Deployments:** Momentan liegt der Fokus auf einer **lokalen Docker-Lösung**, was für Entwicklung und erste Einsätze ideal ist. Langfristig ist jedoch geplant, auch **Cloud-Hosting** anzubieten. Dies könnte in Form eines Managed Service geschehen, bei dem die Infrastruktur vom Anbieter übernommen wird. Hierbei wären dann Hochverfügbarkeit, automatisches Skalieren nach Last und Backup/Recovery-Mechanismen relevant. Auch **On-Premise-Installationen** in Unternehmens-Rechenzentren sollen unterstützt bleiben; hierfür könnten Container-Orchestrierung (Kubernetes Charts) oder VM-Images bereitgestellt werden, die Unternehmen in ihre Umgebung integrieren. In jedem Fall wird das System so weiterentwickelt, dass es sowohl in kleiner Einzelcontainer-Umgebung als auch in großen verteilten Umgebungen performant und zuverlässig läuft.

- **Benutzerfreundliche Web-Oberfläche:** Die zuvor beschriebene geplante Web-UI soll implementiert und iterativ verbessert werden. Neben dem reinen Q&A-Interface könnten Features wie **Highlighting** der Stellen im Dokument, aus denen die Antwort abgeleitet wurde, eingebaut werden. So könnte ein Nutzer z.B. auf die Quellenangabe klicken und sieht dann den entsprechenden Ausschnitt des Originaldokuments hervorgehoben. Außerdem ist an eine **Konfigurationsoberfläche** gedacht, wo man etwa neue Dokumente hochladen oder den Index neu aufbauen kann, ohne auf die CLI zurückgreifen zu müssen. Diese Web-Oberfläche würde das System einem breiteren Nutzerkreis öffnen, der nicht technisch versiert ist.

- **Verbesserte Retrieval- und Ranking-Methoden:** Forschung und Entwicklung im Bereich RAG schreiten schnell voran. Zukünftig könnten wir etwa einen **lernenden Re-Ranker** einsetzen, der mittels maschinellen Lernens (z.B. ein BERT-basiertes Modell) die vom Vektorstore geholten Chunks nochmals bewertet und optimal sortiert. Auch **Feedback-Learning** ist denkbar: das System könnte aus Nutzerfeedback lernen, welche Antworten gut waren, und bei ähnlichen zukünftigen Fragen bevorzugt die damals nützlichen Dokumente hochranken. Ferner könnten **neue Index-Strukturen** (z.B. Hierarchical Navigable Small World-Graphs in neuer Variation) oder **kombinierte Vektor+Symbolic-Suchen** integriert werden, wenn diese sich als vorteilhaft erweisen. Das System soll als Plattform dienen, um solche Innovationen relativ leicht auszuprobieren und einzubinden.

- **Alternative Vektordatenbanken und Modelle:** Auch wenn initial ChromaDB genutzt wird, werden wir beobachten, ob andere Lösungen Vorteile bieten (etwa in der Cloud-Integration oder Performance). Sollte z.B. Pinecone oder Weaviate als Service praktische Vorteile bringen, könnten wir eine Option schaffen, diese anstelle von Chroma zu verwenden – insbesondere im Cloud-Betrieb. Ebenso könnten weitere LLMs integriert werden, falls neue Modelle erscheinen, die attraktive Eigenschaften (bessere Ergebnisse, geringere Kosten, spezifische Domänenkenntnis) bieten. Dank LiteLLM sind wir hier flexibel, neue Anbieter oder lokale Modelle zu unterstützen.

- **Automatisierte Qualitätssicherung:** In Zukunft könnte der Evaluierungsprozess weiter automatisiert werden. Beispielsweise könnte ein kontinuierliches Benchmarking eingerichtet werden, das nach jedem Daten-Update oder Release des Systems einen Satz Testfragen durchläuft und die Metriken (Precision, Recall, etc.) vergleicht. Auffällige Veränderungen würden dann Entwickler alarmieren. Zudem könnten fortgeschrittene Metriken wie der **TruthfulQA-Benchmark** oder **Halluzinations-Detektoren** zum Einsatz kommen, um die Qualität der Antworten noch genauer zu beurteilen. 

Insgesamt ist das System so konzipiert, dass es **mit den Anforderungen mitwachsen** kann. Kurzfristig steht eine robuste Grundfunktionalität im Vordergrund (Dokumente rein, Fragen raus – mit zuverlässigen Antworten). Mittel- und langfristig wird das RAG-System jedoch kontinuierlich erweitert, um neue Anwendungsfälle, größere Datenmengen, mehr Benutzer und neue Technologien abzudecken. Durch die solide modulare Basis, die in diesem PRD beschrieben wurde, sind wir zuversichtlich, dass das System die kommenden Erweiterungen flexibel aufnehmen kann und Entwicklern noch lange als leistungsfähiges **Werkzeug zur Integration von KI und eigenen Daten** dienen wird. 

